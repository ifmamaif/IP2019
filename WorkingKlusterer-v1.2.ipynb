{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOtherClustersKeys(keywords, other_clusters):\n",
    "    \"Gets the keys for all clusters passed in the second parameter\"\n",
    "    \n",
    "    uniqueKeywords = {}\n",
    "    for i in other_clusters:\n",
    "        uniqueKeywords = set(uniqueKeywords).union(set(keywords[i]))\n",
    "    \n",
    "    return uniqueKeywords\n",
    "\n",
    "def getUniqueKeys(clustersNumber, keywords):\n",
    "    \"Used for label creation\"\n",
    "    \n",
    "    unique_keys={}\n",
    "    for cluster in range(clustersNumber):   \n",
    "        other_clusters=list(set(range(clustersNumber))-set([cluster]))\n",
    "        #keys_other_clusters=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "        keys_other_clusters=getOtherClustersKeys(keywords, other_clusters)\n",
    "        unique=set(keywords[cluster])-keys_other_clusters\n",
    "        unique_keys[cluster]=nlargest(10, unique, key=counts[cluster].get)\n",
    "\n",
    "    return unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputs(filePath):\n",
    "    \"This function parses the csv at filepath and returns a list of sentences\"\n",
    "    \n",
    "    inputs = []\n",
    "    with open(filePath, 'r') as readFile: \n",
    "        for line in enumerate(readFile):\n",
    "            inputs += line\n",
    "\n",
    "    curatedInputs = inputs[1::2]\n",
    "    n = len(curatedInputs) -1\n",
    "    for i in range(0, n):\n",
    "        word = curatedInputs[i]\n",
    "        word = word[0:len(word)-1]\n",
    "        curatedInputs[i] = word\n",
    "    \n",
    "        return curatedInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"output.csv\"\n",
    "curatedInputs = getInputs(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,min_df=1,stop_words='english')\n",
    "vectorizerFitting = vectorizer.fit_transform(curatedInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make clusters\n",
    "from sklearn.cluster import KMeans\n",
    "clustersNumber = 20\n",
    "km = KMeans(n_clusters = clustersNumber, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\n",
    "km.fit(vectorizerFitting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster numbering and total words\n",
    "import numpy as np\n",
    "np.unique(km.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign cluster data\n",
    "text={}\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    oneDocument = curatedInputs[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDocument\n",
    "    else:\n",
    "        text[cluster] += oneDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import nltk \n",
    "\n",
    "_stopwords = set(stopwords.words('english') + list(punctuation) + [\"''\",\"``\",\"/i\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\" ])\n",
    "                 \n",
    "keywords={}\n",
    "counts={}\n",
    "for cluster in range(clustersNumber):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100, freq, key=freq.get)\n",
    "    counts[cluster]=freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final output of type key,clusterData                \n",
    "uniqueKeys = getUniqueKeys(clustersNumber, keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRestrictedLabels(usableLabels, kmLabels):\n",
    "    \"Gets a new km.labels_ like object, that can be used to k-nearest classify an input between some but not all the clusters\"\n",
    "        \n",
    "    kmLabels = kmLabels.copy()\n",
    "    for i in range(len(kmLabels)):\n",
    "        label = kmLabels[i]\n",
    "        if label not in usableLabels:\n",
    "            kmLabels[i] = -1\n",
    "        \n",
    "    return kmLabels\n",
    "\n",
    "def getShrinkedCorpus(restrictedLabels, corpus):\n",
    "    \"Shrinks a corpus to relevant labels\"\n",
    "    \n",
    "    restrictedCorpus = []\n",
    "    for i in range(len(restrictedLabels)):\n",
    "        if restrictedLabels[i] != -1:\n",
    "            restrictedCorpus += corpus[i]\n",
    "    \n",
    "    return restrictedCorpus\n",
    "\n",
    "def getShrinkedLabels(restrictedLabels):\n",
    "    \"Shrinks the label list to match the shrinkatinated corpus\"\n",
    "    \n",
    "    shrinkedLabels = []\n",
    "    for i in range(len(restrictedLabels)):\n",
    "        if restrictedLabels[i] != -1:\n",
    "            shrinkedLabels += restrictedLabels[i]\n",
    "    \n",
    "    return shrinkedLabels\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usableLabels = [1, 2, 4, 6]\n",
    "restrictedLabels = getRestrictedLabels(usableLabels, km.labels_)\n",
    "shrinkedCorpus = getShrinkedCorpus(restrictedLabels, vectorizerFitting)\n",
    "shrinkedLabels = getShrinkedLabels(restrictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnearestWrapper:\n",
    "    \n",
    "    def __init__(self, pathToSavedModel, pathToSavedVectorizerFitting, pathToSavedEdgesToRoomsDicti, pathToSavedRoomsToBiasDict):\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        self.__model = [] # @TODO Alex get these please\n",
    "        self.__fitting = [] \n",
    "        self.__biasDict = {}\n",
    "        self.__RoomToEdgesDict = {}\n",
    "    \n",
    "    def __getBias(self, roomId):\n",
    "        return \" \"\n",
    "        # emptyString\n",
    "\n",
    "    def __getEdges(self, roomId):\n",
    "    \n",
    "        #modify this function to look up in the edge dictionary\n",
    "        #this is now hard coded!\n",
    "        edges = []\n",
    "        edge1 = {}\n",
    "        edge1[\"Destination\"] = 1\n",
    "        edge1[\"Labels\"] = [1]\n",
    "    \n",
    "        edges += edge1\n",
    "    \n",
    "        edge2 = {}\n",
    "        edge2[\"Destination\"] = 2\n",
    "        edge3[\"Labels\"] = [2, 4]\n",
    "    \n",
    "        edges += edge2\n",
    "    \n",
    "        edge3 = {}\n",
    "        edge3[\"Destination\"] = 3\n",
    "        edge3[\"Labels\"] = [3,5]\n",
    "    \n",
    "        edges += edge3 \n",
    "    \n",
    "        return edges\n",
    "\n",
    "\n",
    "    def __getAllLabels(edges):\n",
    "        \"Gets all labels within referenced in a list of edges\"\n",
    "    \n",
    "        allLabels = []\n",
    "        for edge in edges:\n",
    "            allLabels += edge[\"Labels\"]\n",
    "    \n",
    "        return allLabels\n",
    "    \n",
    "    def __getUnifiedLabels(shrinkedLabels, edges):\n",
    "        \"Returns a labeling model where each cluster is actually a room\"\n",
    "    \n",
    "        labelToRoomDict = {}\n",
    "    \n",
    "        for edge in edges:\n",
    "            for label in edge[\"Labels\"]:\n",
    "                labelToRoomDict[label] = edge[\"Destination\"]\n",
    "    \n",
    "        unifiedLabels = []\n",
    "        for label in shrinkedLabels:\n",
    "            unifiedLabels.append(labelToRoomDict[label])\n",
    "        \n",
    "        return unifiedLabels\n",
    "\n",
    "\n",
    "\n",
    "    def modifiedKmeans(self, playerInput, roomId):\n",
    "        \"Returns the id of the room it goes to\"\n",
    "    \n",
    "        bias = getBias(roomId)\n",
    "        playerInput = playerInput + bias #string + i assume\n",
    "    \n",
    "        edges = getEdges(roomId)\n",
    "    \n",
    "        usableLabels = getUsableLables[edges]\n",
    "    \n",
    "        restrictedLabels = getRestrictedLabels(usableLabels, allLables)\n",
    "        shrinkedCorpus = getRestrictedCorpus(restrictedLabels, corpus)\n",
    "        shrinkedLabels = getShrinkedLabels(restrictedLabels)\n",
    "    \n",
    "        unifiedLabels = getUnifiedLabels(shrinkedLabels, edges)\n",
    "    \n",
    "        classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "        classifier.fit(shrinkedCorpus,unifiedLabels)\n",
    "    \n",
    "        return classifier.predict(playerInput)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
